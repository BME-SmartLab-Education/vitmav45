# Deep Learning a gyakorlatban Python és LUA alapon
## Második kis házi feladat

A gyakorlatokon ismertetésre került Numpy alapú neurális hálózat kódjába valósítsd meg a következő funkciókat:

1. Mini-batch learning
2. L2 regularizáció (weight decay) és a tanítás során adott számú epochonként mutasd be, hogy hogyan csökkennek a súlyok értékei. Illetve a tanítás végén is mutasd meg, hogy végül tényleg kicsik lettek a súlyok.
3. Drop out

További plusz pontokért leprogramozható feladatok (Csak akkor, ha a másik három pont már készen van!):

4. L1 regularizáció (+10p) és a 2-es ponthoz hasonlóan mutasd be, hogy ritkák (közel nullák) lettek a súlyok
5. Nesterov momentum (+10p)

Az egyes funkciók paramétereit (pl. mini-batch mérete, L2 regularizációs együttható, drop out mértéke, stb.) legyenek külön-külön bekapcsolhatóak, de működjenek együtt is. Ilyenkor figyelni kell az egyes algoritmusok egymásra hatására (pl. drop out esetén a figyelmen kívül hagyott neuronokhoz tartozó súlyokat nem használjuk L1 és L2 regularizációnál).
A forráskód új részeit “# HF2 start eljárás” és “# HF2 end eljárás” (eljárás=minibatch,l2reg,dropout) kommentek közé tedd és kommentbe vagy a notebookba külön írd bele, hogy pontosan mit-miért csináltál.

Csak Github-ra secret gist-ként feltöltött Jupyter notebook formátumot fogadunk el, amelyben a kódok mellett magyarázat és az eredmények is láthatóak. 

Beadási határido: 2017. október 10. éjfél

Beadás: http://smartlab.tmit.bme.hu/oktatas-deep-learning#kishazi 
